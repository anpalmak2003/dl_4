{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba216f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anpalmak/vscode/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/anpalmak/vscode/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f0b9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNDCG(nn.Module):\n",
    "    \"\"\"\n",
    "    NeuralNDCG for single-example list (classes) with gains r (shape [B, C]) and scores s (logits) [B, C].\n",
    "    - Soft permutation via NeuralSort (tau: temperature).\n",
    "    - Supports topk (focus on head of the ranked list).\n",
    "    \"\"\"\n",
    "    def __init__(self, tau: float = 1.0, topk: Optional[int] = None, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "        self.topk = topk\n",
    "        self.eps  = eps\n",
    "\n",
    "    @staticmethod\n",
    "    def _discounts(C: int, device) -> torch.Tensor:\n",
    "        # 1 / log2(1 + rank) ; ranks start at 1\n",
    "        idx = torch.arange(1, C + 1, device=device, dtype=torch.float)\n",
    "        return 1.0 / torch.log2(idx + 1.0)\n",
    "\n",
    "    def _softperm_neuralsort(self, scores: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        NeuralSort: soft permutation matrix \\hat P \\in R^{C x C} for each batch item.\n",
    "        scores: [B, C]\n",
    "        return: P_hat: [B, C, C]\n",
    "        \"\"\"\n",
    "        B, C = scores.shape\n",
    "        # pairwise score diffs (B, C, C)\n",
    "        s = scores.unsqueeze(-1)  # (B,C,1)\n",
    "        diffs = s - s.transpose(1, 2)  # s_i - s_j\n",
    "\n",
    "        # soft ranks via sinkhorn-like normalization (NeuralSort closed form):\n",
    "        # P_hat = softmax(-(C+1-2*range(C)) * scores_sorted / tau) style — используем формулу с |s_i - s_j|.\n",
    "        # Надёжный и простой вариант — «SoftSort by NeuralSort» из оригинальной статьи:\n",
    "        # r_i = sum_j sigmoid((s_j - s_i)/tau)  -> expected rank of item i (1..C)\n",
    "        # Затем из рангов собираем приближенную пермутацию с гаусс-ядером по расстоянию рангов.\n",
    "        # Это «lightweight» приближение; практично и стабильно.\n",
    "\n",
    "        ranks_expect = 1.0 + torch.sum(torch.sigmoid(diffs / self.tau), dim=-1)  # (B,C) in [1..C]\n",
    "        # Гауссово распределим элементы по позициям 1..C\n",
    "        pos = torch.arange(1, C + 1, device=scores.device).view(1, 1, C)  # (1,1,C)\n",
    "        # bandwidth — эмпирически берем tau*C/2\n",
    "        bw = self.tau * C / 2.0 + 1e-6\n",
    "        P = torch.exp(- (ranks_expect.unsqueeze(-1) - pos) ** 2 / (2 * bw**2))  # (B,C,C)\n",
    "        P = P / (P.sum(dim=-1, keepdim=True) + self.eps)\n",
    "        return P  # rows: items, cols: positions\n",
    "\n",
    "    def forward(self, scores: torch.Tensor, gains: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        scores: [B, C] (logits)\n",
    "        gains:  [B, C] (e.g., one-hot for single-label or graded relevance for multi-label)\n",
    "        \"\"\"\n",
    "        B, C = scores.shape\n",
    "        P_hat = self._softperm_neuralsort(scores)  # [B, C, C]\n",
    "\n",
    "        # Discounts for positions (col dimension)\n",
    "        if self.topk is None:\n",
    "            K = C\n",
    "        else:\n",
    "            K = min(self.topk, C)\n",
    "        D = self._discounts(C, scores.device)  # [C]\n",
    "        D = D[:K]  # take head if topk set\n",
    "\n",
    "        # Expected DCG: sum_i sum_pos gains_i * P[i, pos] * D[pos]\n",
    "        # P_hat rows: items; cols: positions\n",
    "        P_head = P_hat[:, :, :K]  # [B, C, K]\n",
    "        dcg = torch.sum(gains.unsqueeze(-1) * P_head * D.view(1, 1, K), dim=(1, 2))  # [B]\n",
    "\n",
    "        # Ideal DCG (hard sort of gains descending)\n",
    "        sorted_gains, _ = torch.sort(gains, dim=-1, descending=True)\n",
    "        idcg = torch.sum(sorted_gains[:, :K] * D.view(1, K), dim=-1) + self.eps  # [B]\n",
    "\n",
    "        ndcg = dcg / idcg\n",
    "        # we minimize 1 - NDCG (maximize NDCG)\n",
    "        loss = 1.0 - ndcg\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e5267c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "DATASETS = {\n",
    "    \"sentiment_en\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n",
    "    \"topic_en\":     (\"ag_news\", None, \"text\", \"label\"),\n",
    "}\n",
    "\n",
    "def _to_str_list(col) -> list[str]:\n",
    "    out = []\n",
    "    for x in col:\n",
    "        if x is None:\n",
    "            out.append(\"\")\n",
    "        elif isinstance(x, str):\n",
    "            out.append(x)\n",
    "        elif isinstance(x, (bytes, bytearray)):\n",
    "            out.append(x.decode(\"utf-8\", errors=\"ignore\"))\n",
    "        elif isinstance(x, (list, tuple)):\n",
    "            out.append(\" \".join(map(str, x)))\n",
    "        elif isinstance(x, dict):\n",
    "            out.append(\" \".join(map(str, x.values())))\n",
    "        else:\n",
    "            out.append(str(x))\n",
    "    return out\n",
    "\n",
    "def load_text_classification(task: str,\n",
    "                             limit_train: Optional[int] = None,\n",
    "                             limit_eval: Optional[int] = None):\n",
    "    name, subset, text_col, y_col = DATASETS[task]\n",
    "    ds = load_dataset(name) if subset is None else load_dataset(name, subset)\n",
    "\n",
    "    tr = ds[\"train\"] if \"train\" in ds else ds[\"validation\"]\n",
    "    te = ds[\"test\"]  if \"test\"  in ds else ds[\"validation\"]\n",
    "\n",
    "    if limit_train: tr = tr.select(range(min(limit_train, len(tr))))\n",
    "    if limit_eval:  te = te.select(range(min(limit_eval,  len(te))))\n",
    "\n",
    "    texts_tr = _to_str_list(tr[text_col])\n",
    "    texts_te = _to_str_list(te[text_col])\n",
    "    y_tr = list(map(int, tr[y_col]))\n",
    "    y_te = list(map(int, te[y_col]))\n",
    "    num_labels = len(set(y_tr))\n",
    "    return (texts_tr, y_tr), (texts_te, y_te), num_labels\n",
    "\n",
    "@dataclass\n",
    "class EncodedDataset(torch.utils.data.Dataset):\n",
    "    enc: Dict[str, torch.Tensor]\n",
    "    y: torch.Tensor\n",
    "    def __len__(self): return self.y.size(0)\n",
    "    def __getitem__(self, i):\n",
    "        item = {k: v[i] for k, v in self.enc.items()}\n",
    "        item[\"labels\"] = self.y[i]\n",
    "        return item\n",
    "\n",
    "def make_encoded_dataset(texts: list[str], labels: list[int]):\n",
    "    texts = _to_str_list(texts)\n",
    "    enc = tokenizer(texts, padding=True, truncation=True,\n",
    "                    max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    y = torch.tensor(list(map(int, labels)), dtype=torch.long)\n",
    "    return EncodedDataset(enc, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"  # можно поменять на \"ai-forever/ruBert-base\"\n",
    "MAX_LEN    = 160\n",
    "BATCH      = 32\n",
    "EPOCHS     = 3\n",
    "LR         = 2e-5\n",
    "LAMBDA_NDCG = 0.5   # вес NeuralNDCG\n",
    "TOPK        = 3     # фокус на top-k (идея «усилить голову распределения»)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "@dataclass\n",
    "class EncodedDataset(torch.utils.data.Dataset):\n",
    "    enc: Dict[str, torch.Tensor]\n",
    "    y: torch.Tensor\n",
    "\n",
    "    def __len__(self): return self.y.size(0)\n",
    "    def __getitem__(self, i):\n",
    "        item = {k: v[i] for k, v in self.enc.items()}\n",
    "        item[\"labels\"] = self.y[i]\n",
    "        return item\n",
    "\n",
    "def make_encoded_dataset(texts: List[str], labels: List[int]):\n",
    "    enc = tokenizer(\n",
    "        texts, padding=True, truncation=True, max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "        # return_token_type_ids=False  # можно явно выключить для DistilBERT\n",
    "    )\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    return EncodedDataset(enc, y)\n",
    "\n",
    "def build_model(num_labels: int):\n",
    "    m = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a408c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(logits: torch.Tensor, y: torch.Tensor, k: int = 3) -> float:\n",
    "    topk = logits.topk(k, dim=-1).indices  # [B,k]\n",
    "    y = y.view(-1, 1).expand_as(topk)\n",
    "    hit = (topk == y).any(dim=-1).float().mean().item()\n",
    "    return hit\n",
    "\n",
    "def ndcg_at_k(logits: torch.Tensor, y: torch.Tensor, k: int = 3) -> float:\n",
    "    # relevance = one-hot\n",
    "    B, C = logits.shape\n",
    "    gains = torch.zeros_like(logits)\n",
    "    gains[torch.arange(B), y] = 1.0\n",
    "    # hard NDCG@k:\n",
    "    sorted_logits, idx = torch.sort(logits, dim=-1, descending=True)\n",
    "    sorted_gains = torch.gather(gains, dim=-1, index=idx)\n",
    "    D = 1.0 / torch.log2(torch.arange(1, C+1, device=logits.device).float() + 1.0)\n",
    "    dcg  = (sorted_gains[:, :k] * D[:k]).sum(dim=-1)\n",
    "    idcg = (torch.sort(gains, dim=-1, descending=True).values[:, :k] * D[:k]).sum(dim=-1) + 1e-8\n",
    "    return (dcg / idcg).mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddef1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(task=\"sentiment_en\", use_neural_ndcg=True, seed=42):\n",
    "    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
    "\n",
    "    (Xtr, ytr), (Xte, yte), C = load_text_classification(task, limit_train=None, limit_eval=None)\n",
    "    ds_tr = make_encoded_dataset(Xtr, ytr)\n",
    "    ds_te = make_encoded_dataset(Xte, yte)\n",
    "    tr_loader = DataLoader(ds_tr, batch_size=BATCH, shuffle=True)\n",
    "    te_loader = DataLoader(ds_te, batch_size=BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model  = build_model(C).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    num_steps = EPOCHS * len(tr_loader)\n",
    "    sch = get_linear_schedule_with_warmup(opt, int(0.1*num_steps), num_steps)\n",
    "\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    ndcg_loss = NeuralNDCG(tau=1.0, topk=TOPK)\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        tr_loss, tr_ce, tr_ndcg = 0.0, 0.0, 0.0\n",
    "        for batch in tr_loader:\n",
    "            batch = {k: v.to(device) for k,v in batch.items()}\n",
    "            y = batch.pop(\"labels\")\n",
    "            out = model(**batch)  # logits\n",
    "            logits = out.logits\n",
    "\n",
    "            loss_ce = ce(logits, y)\n",
    "            if use_neural_ndcg:\n",
    "                # формы [B,C]\n",
    "                gains = torch.zeros_like(logits).scatter_(1, y.unsqueeze(1), 1.0)\n",
    "                loss_ndcg = ndcg_loss(logits, gains)\n",
    "                loss = loss_ce + LAMBDA_NDCG * loss_ndcg\n",
    "                tr_ndcg += loss_ndcg.item() * y.size(0)\n",
    "            else:\n",
    "                loss = loss_ce\n",
    "\n",
    "            tr_ce   += loss_ce.item() * y.size(0)\n",
    "            tr_loss += loss.item()   * y.size(0)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step(); sch.step()\n",
    "\n",
    "        # валидация\n",
    "        model.eval()\n",
    "        all_y, all_p = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in te_loader:\n",
    "                labels = batch.pop(\"labels\")\n",
    "                batch = {k: v.to(device) for k,v in batch.items()}\n",
    "                out = model(**batch)\n",
    "                logits = out.logits.detach().cpu()\n",
    "                preds  = logits.argmax(dim=-1)\n",
    "                all_y.append(labels)\n",
    "                all_p.append(logits)\n",
    "        y_true = torch.cat(all_y)\n",
    "        logits = torch.cat(all_p)\n",
    "\n",
    "        acc = accuracy_score(y_true, logits.argmax(dim=-1))\n",
    "        f1  = f1_score(y_true, logits.argmax(dim=-1), average=\"macro\")\n",
    "        p_at_k = precision_at_k(logits, y_true, k=TOPK)\n",
    "        ndcg_k = ndcg_at_k(logits, y_true, k=TOPK)\n",
    "\n",
    "        n = len(ds_tr)\n",
    "        msg = (f\"[{task}] epoch {ep}/{EPOCHS} | \"\n",
    "               f\"train_loss={tr_loss/n:.4f} (ce={tr_ce/n:.4f}, ndcg={tr_ndcg/n:.4f}) | \"\n",
    "               f\"Acc={acc:.4f} F1={f1:.4f} P@{TOPK}={p_at_k:.4f} NDCG@{TOPK}={ndcg_k:.4f}\")\n",
    "        print(msg)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a329ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class SimpleCLSHead(nn.Module):\n",
    "    def __init__(self, model_name: str, num_labels: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden, num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, **kwargs):\n",
    "        # Универсально: передаём только те тензоры, которые есть\n",
    "        inputs = {}\n",
    "        if input_ids is not None: inputs[\"input_ids\"] = input_ids\n",
    "        if attention_mask is not None: inputs[\"attention_mask\"] = attention_mask\n",
    "        # Некоторые модели (DistilBERT) не используют token_type_ids — просто не передаём\n",
    "        if token_type_ids is not None and \"token_type_ids\" in self.backbone.forward.__code__.co_varnames:\n",
    "            inputs[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "        out = self.backbone(**inputs)                    # last_hidden_state: [B, T, H]\n",
    "        cls = out.last_hidden_state[:, 0]               # CLS\n",
    "        logits = self.classifier(self.dropout(cls))     # [B, C]\n",
    "\n",
    "        # Возвращаем объект с .logits, чтобы остальной код не трогать\n",
    "        return type(\"Out\", (), {\"logits\": logits})\n",
    "\n",
    "def build_model(num_labels: int):\n",
    "    # Использует твой MODEL_NAME\n",
    "    return SimpleCLSHead(MODEL_NAME, num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be8336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE (CrossEntropy) ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find BertModel neither in <module 'transformers.models.bert' from '/Users/anpalmak/vscode/.venv/lib/python3.9/site-packages/transformers/models/bert/__init__.py'> nor in <module 'transformers' from '/Users/anpalmak/vscode/.venv/lib/python3.9/site-packages/transformers/__init__.py'>!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/vscode/.venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:741\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "File \u001b[0;32m~/vscode/.venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:745\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find BertModel in <module 'transformers' from '/Users/anpalmak/vscode/.venv/lib/python3.9/site-packages/transformers/__init__.py'>!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m all_hist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== BASELINE (CrossEntropy) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m _, h1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_with_history\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment_en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_neural_ndcg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m _, h2 \u001b[38;5;241m=\u001b[39m train_one_with_history(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic_en\u001b[39m\u001b[38;5;124m\"\u001b[39m,     use_neural_ndcg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== CE + NeuralNDCG (λ=\u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m, topk=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (LAMBDA_NDCG, TOPK))\n",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m, in \u001b[0;36mtrain_one_with_history\u001b[0;34m(task, use_neural_ndcg, seed)\u001b[0m\n\u001b[1;32m      9\u001b[0m te_loader \u001b[38;5;241m=\u001b[39m DataLoader(ds_te, batch_size\u001b[38;5;241m=\u001b[39mBATCH)\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m model  \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR)\n\u001b[1;32m     15\u001b[0m num_steps \u001b[38;5;241m=\u001b[39m EPOCHS \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(tr_loader)\n",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(num_labels)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_model\u001b[39m(num_labels: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Использует твой MODEL_NAME\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSimpleCLSHead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m, in \u001b[0;36mSimpleCLSHead.__init__\u001b[0;34m(self, model_name, num_labels, dropout)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m, num_labels: \u001b[38;5;28mint\u001b[39m, dropout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(dropout)\n",
      "File \u001b[0;32m~/vscode/.venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:601\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    598\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    599\u001b[0m     )\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[0;32m--> 601\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n",
      "File \u001b[0;32m~/vscode/.venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:394\u001b[0m, in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[0;32m--> 394\u001b[0m     supported_models \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "File \u001b[0;32m~/vscode/.venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:807\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[1;32m    806\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[1;32m    810\u001b[0m model_types \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m key\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[0;32m~/vscode/.venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:821\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vscode/.venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:743\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m getattribute_from_module(transformers_module, attr)\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m--> 743\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m neither in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nor in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find BertModel neither in <module 'transformers.models.bert' from '/Users/anpalmak/vscode/.venv/lib/python3.9/site-packages/transformers/models/bert/__init__.py'> nor in <module 'transformers' from '/Users/anpalmak/vscode/.venv/lib/python3.9/site-packages/transformers/__init__.py'>!"
     ]
    }
   ],
   "source": [
    "# ===== ХЕЛПЕР: сохраняем историю метрик из train_one =====\n",
    "def train_one_with_history(task=\"sentiment_en\", use_neural_ndcg=True, seed=42):\n",
    "    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
    "\n",
    "    (Xtr, ytr), (Xte, yte), C = load_text_classification(task, limit_train=None, limit_eval=None)\n",
    "    ds_tr = make_encoded_dataset(Xtr, ytr)\n",
    "    ds_te = make_encoded_dataset(Xte, yte)\n",
    "    tr_loader = DataLoader(ds_tr, batch_size=BATCH, shuffle=True)\n",
    "    te_loader = DataLoader(ds_te, batch_size=BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model  = build_model(C).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    num_steps = EPOCHS * len(tr_loader)\n",
    "    sch = get_linear_schedule_with_warmup(opt, int(0.1*num_steps), num_steps)\n",
    "\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    ndcg_loss = NeuralNDCG(tau=1.0, topk=TOPK)\n",
    "\n",
    "    history = []  # тут копим метрики по эпохам\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        tr_loss, tr_ce, tr_ndcg = 0.0, 0.0, 0.0\n",
    "        n_seen = 0\n",
    "\n",
    "        for batch in tr_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            y = batch.pop(\"labels\")\n",
    "            out = model(**batch)\n",
    "            logits = out.logits\n",
    "\n",
    "            loss_ce = ce(logits, y)\n",
    "            if use_neural_ndcg:\n",
    "                gains = torch.zeros_like(logits).scatter_(1, y.unsqueeze(1), 1.0)\n",
    "                loss_ndcg = ndcg_loss(logits, gains)\n",
    "                loss = loss_ce + LAMBDA_NDCG * loss_ndcg\n",
    "                tr_ndcg += loss_ndcg.item() * y.size(0)\n",
    "            else:\n",
    "                loss = loss_ce\n",
    "\n",
    "            tr_ce   += loss_ce.item() * y.size(0)\n",
    "            tr_loss += loss.item()   * y.size(0)\n",
    "            n_seen  += y.size(0)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step(); sch.step()\n",
    "\n",
    "        model.eval()\n",
    "        all_y, all_logits = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in te_loader:\n",
    "                labels = batch.pop(\"labels\")\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                out = model(**batch)\n",
    "                logits = out.logits.detach().cpu()\n",
    "                all_y.append(labels); all_logits.append(logits)\n",
    "\n",
    "        y_true = torch.cat(all_y)\n",
    "        logits = torch.cat(all_logits)\n",
    "        acc = accuracy_score(y_true, logits.argmax(dim=-1))\n",
    "        f1  = f1_score(y_true, logits.argmax(dim=-1), average=\"macro\")\n",
    "        p_at_k = precision_at_k(logits, y_true, k=TOPK)\n",
    "        ndcg_k = ndcg_at_k(logits, y_true, k=TOPK)\n",
    "\n",
    "        history.append({\n",
    "            \"task\": task,\n",
    "            \"mode\": \"CE+NeuralNDCG\" if use_neural_ndcg else \"CE\",\n",
    "            \"epoch\": ep,\n",
    "            \"train_loss\": tr_loss / max(1, n_seen),\n",
    "            \"train_ce\":   tr_ce   / max(1, n_seen),\n",
    "            \"train_ndcg\": tr_ndcg / max(1, n_seen) if use_neural_ndcg else 0.0,\n",
    "            \"acc\": acc, \"f1\": f1, f\"P@{TOPK}\": p_at_k, f\"NDCG@{TOPK}\": ndcg_k\n",
    "        })\n",
    "\n",
    "        print(f\"[{task}][{'ND' if use_neural_ndcg else 'CE'}] \"\n",
    "              f\"epoch {ep}/{EPOCHS} | \"\n",
    "              f\"loss={history[-1]['train_loss']:.4f} \"\n",
    "              f\"(ce={history[-1]['train_ce']:.4f}, nd={history[-1]['train_ndcg']:.4f}) | \"\n",
    "              f\"Acc={acc:.4f} F1={f1:.4f} P@{TOPK}={p_at_k:.4f} NDCG@{TOPK}={ndcg_k:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# ===== ЗАПУСК ЭКСПЕРИМЕНТОВ =====\n",
    "all_hist = []\n",
    "\n",
    "print(\"=== BASELINE (CrossEntropy) ===\")\n",
    "_, h1 = train_one_with_history(\"sentiment_en\", use_neural_ndcg=False)\n",
    "_, h2 = train_one_with_history(\"topic_en\",     use_neural_ndcg=False)\n",
    "all_hist += h1 + h2\n",
    "\n",
    "print(\"\\n=== CE + NeuralNDCG (λ=%.2f, topk=%d) ===\" % (LAMBDA_NDCG, TOPK))\n",
    "_, h3 = train_one_with_history(\"sentiment_en\", use_neural_ndcg=True)\n",
    "_, h4 = train_one_with_history(\"topic_en\",     use_neural_ndcg=True)\n",
    "all_hist += h3 + h4\n",
    "\n",
    "# ===== СВОДНАЯ ТАБЛИЦА =====\n",
    "import pandas as pd\n",
    "df_hist = pd.DataFrame(all_hist)\n",
    "display(df_hist)\n",
    "\n",
    "# Итог по лучшей эпохе для каждой (task, mode)\n",
    "summary = (df_hist\n",
    "           .sort_values([\"task\",\"mode\",\"f1\"], ascending=[True, True, False])\n",
    "           .groupby([\"task\",\"mode\"], as_index=False)\n",
    "           .first()[[\"task\",\"mode\",\"epoch\",\"acc\",\"f1\",f\"P@{TOPK}\",f\"NDCG@{TOPK}\"]])\n",
    "print(\"\\n==== SUMMARY (best epoch per task/mode) ====\")\n",
    "display(summary)\n",
    "\n",
    "# ===== ГРАФИКИ (динамика по эпохам) =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for task in df_hist[\"task\"].unique():\n",
    "    sub = df_hist[df_hist[\"task\"]==task]\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for mode in [\"CE\", \"CE+NeuralNDCG\"]:\n",
    "        sm = sub[sub[\"mode\"]==mode]\n",
    "        plt.plot(sm[\"epoch\"], sm[\"f1\"], marker=\"o\", label=f\"{mode} F1\")\n",
    "        plt.plot(sm[\"epoch\"], sm[f\"NDCG@{TOPK}\"], marker=\"s\", linestyle=\"--\", label=f\"{mode} NDCG@{TOPK}\")\n",
    "    plt.title(f\"{task}: F1 & NDCG@{TOPK} by epoch\")\n",
    "    plt.xlabel(\"epoch\"); plt.grid(True); plt.legend(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
